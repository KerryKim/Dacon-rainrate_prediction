학습률 = 3e-5

    nfolds = 5              #if nfolds=1, code not work (temp_train_fold is all train data)
    test_nfolds = 1
    nfilters = 128
    epochs = 50
    batch_train = 32
    batch_valid = 128
    ensemble = 0
    img_row_size, img_col_size = 40, 40






C:\Users\jh902\anaconda3\envs\cuda\python.exe C:/Users/jh902/PycharmProjects/rain/baseline_autoencoder.py
test: 100%|██████████| 2416/2416 [00:01<00:00, 1621.24it/s]
2020-05-14 20:09:40.410725: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-05-14 20:09:40.554239: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2020-05-14 20:09:40.764439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815
pciBusID: 0000:08:00.0
totalMemory: 8.00GiB freeMemory: 6.56GiB
2020-05-14 20:09:40.764593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2020-05-14 20:09:41.186152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 20:09:41.186240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2020-05-14 20:09:41.186286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2020-05-14 20:09:41.186412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6294 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 40, 40, 9)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 40, 40, 128)  10496       input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 40, 40, 128)  147584      conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_v2 (BatchNo (None, 40, 40, 128)  512         conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 20, 20, 128)  0           batch_normalization_v2[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 20, 20, 128)  0           max_pooling2d[0][0]              
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 20, 20, 256)  295168      dropout[0][0]                    
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 20, 256)  590080      conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v2_1 (Batch (None, 20, 20, 256)  1024        conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 256)  0           batch_normalization_v2_1[0][0]   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 10, 10, 256)  0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 10, 10, 512)  1180160     dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 10, 10, 512)  2359808     conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v2_2 (Batch (None, 10, 10, 512)  2048        conv2d_5[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 512)    0           batch_normalization_v2_2[0][0]   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 5, 5, 512)    0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 5, 5, 1024)   4719616     dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 10, 10, 512)  4719104     conv2d_6[0][0]                   
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 10, 10, 1024) 0           conv2d_transpose[0][0]           
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 10, 10, 1024) 0           concatenate[0][0]                
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 10, 10, 512)  4719104     dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 10, 10, 512)  2359808     conv2d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v2_3 (Batch (None, 10, 10, 512)  2048        conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 20, 20, 256)  1179904     batch_normalization_v2_3[0][0]   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 20, 20, 512)  0           conv2d_transpose_1[0][0]         
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 20, 20, 512)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 20, 256)  1179904     dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v2_4 (Batch (None, 20, 20, 256)  1024        conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 40, 40, 128)  295040      batch_normalization_v2_4[0][0]   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 40, 40, 256)  0           conv2d_transpose_2[0][0]         
                                                                 conv2d_1[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 40, 40, 256)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 40, 128)  295040      dropout_5[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 40, 40, 128)  147584      conv2d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v2_5 (Batch (None, 40, 40, 128)  512         conv2d_12[0][0]                  
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 40, 40, 128)  0           batch_normalization_v2_5[0][0]   
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 40, 40, 1)    129         dropout_6[0][0]                  
==================================================================================================
Total params: 24,795,777
Trainable params: 24,792,193
Non-trainable params: 3,584
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\jh902\anaconda3\envs\cuda\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
=====Train Model_0 iteration====
Epoch 1/50
1710/1710 [==============================] - 605s 354ms/step - loss: 0.2715 - maeOverFscore_keras: 3.0265 - fscore_keras: 0.4749 - val_loss: 0.0000e+00 - val_maeOverFscore_keras: 0.0000e+00 - val_fscore_keras: 0.0000e+00
Epoch 2/50
1710/1710 [==============================] - 222s 130ms/step - loss: 0.2157 - maeOverFscore_keras: 2.3505 - fscore_keras: 0.5523 - val_loss: 0.2108 - val_maeOverFscore_keras: 2.1562 - val_fscore_keras: 0.5878
Epoch 3/50
1710/1710 [==============================] - 221s 129ms/step - loss: 0.2110 - maeOverFscore_keras: 2.2439 - fscore_keras: 0.5648 - val_loss: 0.2107 - val_maeOverFscore_keras: 2.1343 - val_fscore_keras: 0.5921
Epoch 4/50
1710/1710 [==============================] - 222s 130ms/step - loss: 0.2076 - maeOverFscore_keras: 2.1750 - fscore_keras: 0.5734 - val_loss: 0.2082 - val_maeOverFscore_keras: 2.2183 - val_fscore_keras: 0.5703
Epoch 5/50
1710/1710 [==============================] - 222s 130ms/step - loss: 0.2076 - maeOverFscore_keras: 2.1855 - fscore_keras: 0.5720 - val_loss: 0.2048 - val_maeOverFscore_keras: 2.1386 - val_fscore_keras: 0.5787
Epoch 6/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.2041 - maeOverFscore_keras: 2.0883 - fscore_keras: 0.5868 - val_loss: 0.2041 - val_maeOverFscore_keras: 2.1497 - val_fscore_keras: 0.5759
Epoch 7/50
1710/1710 [==============================] - 220s 128ms/step - loss: 0.2022 - maeOverFscore_keras: 2.0431 - fscore_keras: 0.5933 - val_loss: 0.2051 - val_maeOverFscore_keras: 2.2099 - val_fscore_keras: 0.5647
Epoch 8/50
1710/1710 [==============================] - 221s 129ms/step - loss: 0.2053 - maeOverFscore_keras: 2.1340 - fscore_keras: 0.5795 - val_loss: 0.2038 - val_maeOverFscore_keras: 2.1634 - val_fscore_keras: 0.5708
Epoch 9/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.2005 - maeOverFscore_keras: 1.9988 - fscore_keras: 0.6006 - val_loss: 0.1997 - val_maeOverFscore_keras: 1.9938 - val_fscore_keras: 0.6036
Epoch 10/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1986 - maeOverFscore_keras: 1.9531 - fscore_keras: 0.6082 - val_loss: 0.1968 - val_maeOverFscore_keras: 1.8225 - val_fscore_keras: 0.6422
Epoch 11/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1972 - maeOverFscore_keras: 1.9223 - fscore_keras: 0.6138 - val_loss: 0.1963 - val_maeOverFscore_keras: 1.7984 - val_fscore_keras: 0.6486
Epoch 12/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1974 - maeOverFscore_keras: 1.9255 - fscore_keras: 0.6140 - val_loss: 0.1946 - val_maeOverFscore_keras: 1.7261 - val_fscore_keras: 0.6628
Epoch 13/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1946 - maeOverFscore_keras: 1.8660 - fscore_keras: 0.6235 - val_loss: 0.1946 - val_maeOverFscore_keras: 1.7985 - val_fscore_keras: 0.6461
Epoch 14/50
1710/1710 [==============================] - 219s 128ms/step - loss: 0.1934 - maeOverFscore_keras: 1.8429 - fscore_keras: 0.6273 - val_loss: 0.1963 - val_maeOverFscore_keras: 1.8847 - val_fscore_keras: 0.6253
Epoch 15/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1923 - maeOverFscore_keras: 1.8208 - fscore_keras: 0.6314 - val_loss: 0.1946 - val_maeOverFscore_keras: 1.8120 - val_fscore_keras: 0.6418
Epoch 16/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1911 - maeOverFscore_keras: 1.7992 - fscore_keras: 0.6349 - val_loss: 0.1943 - val_maeOverFscore_keras: 1.8060 - val_fscore_keras: 0.6426
Epoch 17/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1932 - maeOverFscore_keras: 1.8454 - fscore_keras: 0.6270 - val_loss: 0.1923 - val_maeOverFscore_keras: 1.7147 - val_fscore_keras: 0.6639
Epoch 18/50
1710/1710 [==============================] - 220s 128ms/step - loss: 0.1896 - maeOverFscore_keras: 1.7765 - fscore_keras: 0.6385 - val_loss: 0.1922 - val_maeOverFscore_keras: 1.7017 - val_fscore_keras: 0.6680
Epoch 19/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1886 - maeOverFscore_keras: 1.7589 - fscore_keras: 0.6413 - val_loss: 0.1904 - val_maeOverFscore_keras: 1.6425 - val_fscore_keras: 0.6803
Epoch 20/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1875 - maeOverFscore_keras: 1.7418 - fscore_keras: 0.6440 - val_loss: 0.1951 - val_maeOverFscore_keras: 1.8089 - val_fscore_keras: 0.6424
Epoch 21/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1865 - maeOverFscore_keras: 1.7266 - fscore_keras: 0.6462 - val_loss: 0.1911 - val_maeOverFscore_keras: 1.7051 - val_fscore_keras: 0.6686
Epoch 22/50
1710/1710 [==============================] - 220s 128ms/step - loss: 0.1857 - maeOverFscore_keras: 1.7122 - fscore_keras: 0.6489 - val_loss: 0.1918 - val_maeOverFscore_keras: 1.6158 - val_fscore_keras: 0.6882
Epoch 23/50
1710/1710 [==============================] - 220s 129ms/step - loss: 0.1848 - maeOverFscore_keras: 1.6992 - fscore_keras: 0.6508 - val_loss: 0.1907 - val_maeOverFscore_keras: 1.7560 - val_fscore_keras: 0.6507
Epoch 24/50
1710/1710 [==============================] - 220s 128ms/step - loss: 0.1867 - maeOverFscore_keras: 1.7483 - fscore_keras: 0.6413 - val_loss: 0.1926 - val_maeOverFscore_keras: 1.6440 - val_fscore_keras: 0.6822
=====Train Model_1 iteration====
Epoch 1/50
1710/1710 [==============================] - 232s 136ms/step - loss: 0.1843 - maeOverFscore_keras: 1.6998 - fscore_keras: 0.6493 - val_loss: 0.0000e+00 - val_maeOverFscore_keras: 0.0000e+00 - val_fscore_keras: 0.0000e+00
Epoch 2/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1829 - maeOverFscore_keras: 1.6745 - fscore_keras: 0.6542 - val_loss: 0.1911 - val_maeOverFscore_keras: 1.6457 - val_fscore_keras: 0.6827
Epoch 3/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1826 - maeOverFscore_keras: 1.6730 - fscore_keras: 0.6537 - val_loss: 0.1918 - val_maeOverFscore_keras: 1.6909 - val_fscore_keras: 0.6730
Epoch 4/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1811 - maeOverFscore_keras: 1.6467 - fscore_keras: 0.6584 - val_loss: 0.1890 - val_maeOverFscore_keras: 1.6351 - val_fscore_keras: 0.6840
Epoch 5/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1802 - maeOverFscore_keras: 1.6350 - fscore_keras: 0.6600 - val_loss: 0.1916 - val_maeOverFscore_keras: 1.5867 - val_fscore_keras: 0.6957
Epoch 6/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1797 - maeOverFscore_keras: 1.6273 - fscore_keras: 0.6612 - val_loss: 0.1906 - val_maeOverFscore_keras: 1.6008 - val_fscore_keras: 0.6934
Epoch 7/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1788 - maeOverFscore_keras: 1.6140 - fscore_keras: 0.6634 - val_loss: 0.1915 - val_maeOverFscore_keras: 1.6600 - val_fscore_keras: 0.6769
Epoch 8/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1781 - maeOverFscore_keras: 1.6062 - fscore_keras: 0.6642 - val_loss: 0.1912 - val_maeOverFscore_keras: 1.5855 - val_fscore_keras: 0.6933
Epoch 9/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1785 - maeOverFscore_keras: 1.6160 - fscore_keras: 0.6618 - val_loss: 0.1900 - val_maeOverFscore_keras: 1.6521 - val_fscore_keras: 0.6799
=====Train Model_2 iteration====
Epoch 1/50
1710/1710 [==============================] - 232s 136ms/step - loss: 0.1761 - maeOverFscore_keras: 1.5828 - fscore_keras: 0.6669 - val_loss: 0.0000e+00 - val_maeOverFscore_keras: 0.0000e+00 - val_fscore_keras: 0.0000e+00
Epoch 2/50
1710/1710 [==============================] - 228s 133ms/step - loss: 0.1753 - maeOverFscore_keras: 1.5705 - fscore_keras: 0.6689 - val_loss: 0.1901 - val_maeOverFscore_keras: 1.6242 - val_fscore_keras: 0.6850
Epoch 3/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1744 - maeOverFscore_keras: 1.5591 - fscore_keras: 0.6706 - val_loss: 0.1907 - val_maeOverFscore_keras: 1.5983 - val_fscore_keras: 0.6926
Epoch 4/50
1710/1710 [==============================] - 232s 135ms/step - loss: 0.1737 - maeOverFscore_keras: 1.5495 - fscore_keras: 0.6716 - val_loss: 0.1906 - val_maeOverFscore_keras: 1.6824 - val_fscore_keras: 0.6711
Epoch 5/50
1710/1710 [==============================] - 241s 141ms/step - loss: 0.1730 - maeOverFscore_keras: 1.5435 - fscore_keras: 0.6721 - val_loss: 0.1886 - val_maeOverFscore_keras: 1.6262 - val_fscore_keras: 0.6822
Epoch 6/50
1710/1710 [==============================] - 241s 141ms/step - loss: 0.1723 - maeOverFscore_keras: 1.5335 - fscore_keras: 0.6736 - val_loss: 0.1898 - val_maeOverFscore_keras: 1.6265 - val_fscore_keras: 0.6848
Epoch 7/50
1710/1710 [==============================] - 342s 200ms/step - loss: 0.1718 - maeOverFscore_keras: 1.5270 - fscore_keras: 0.6745 - val_loss: 0.1895 - val_maeOverFscore_keras: 1.6449 - val_fscore_keras: 0.6786
Epoch 8/50
1710/1710 [==============================] - 343s 200ms/step - loss: 0.1718 - maeOverFscore_keras: 1.5380 - fscore_keras: 0.6730 - val_loss: 0.1874 - val_maeOverFscore_keras: 1.5922 - val_fscore_keras: 0.6903
Epoch 9/50
1710/1710 [==============================] - 259s 152ms/step - loss: 0.1705 - maeOverFscore_keras: 1.5112 - fscore_keras: 0.6764 - val_loss: 0.1920 - val_maeOverFscore_keras: 1.6882 - val_fscore_keras: 0.6725
Epoch 10/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1709 - maeOverFscore_keras: 1.5141 - fscore_keras: 0.6767 - val_loss: 0.1977 - val_maeOverFscore_keras: 1.6561 - val_fscore_keras: 0.6820
Epoch 11/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1702 - maeOverFscore_keras: 1.5057 - fscore_keras: 0.6777 - val_loss: 0.1923 - val_maeOverFscore_keras: 1.6127 - val_fscore_keras: 0.6898
Epoch 12/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1701 - maeOverFscore_keras: 1.5084 - fscore_keras: 0.6764 - val_loss: 0.1892 - val_maeOverFscore_keras: 1.5716 - val_fscore_keras: 0.6961
Epoch 13/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1690 - maeOverFscore_keras: 1.4910 - fscore_keras: 0.6794 - val_loss: 0.1998 - val_maeOverFscore_keras: 1.7040 - val_fscore_keras: 0.6784
=====Train Model_3 iteration====
Epoch 1/50
1710/1710 [==============================] - 234s 137ms/step - loss: 0.1672 - maeOverFscore_keras: 1.4731 - fscore_keras: 0.6811 - val_loss: 0.0000e+00 - val_maeOverFscore_keras: 0.0000e+00 - val_fscore_keras: 0.0000e+00
Epoch 2/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1664 - maeOverFscore_keras: 1.4625 - fscore_keras: 0.6826 - val_loss: 0.1896 - val_maeOverFscore_keras: 1.5694 - val_fscore_keras: 0.6954
Epoch 3/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1658 - maeOverFscore_keras: 1.4562 - fscore_keras: 0.6831 - val_loss: 0.1940 - val_maeOverFscore_keras: 1.6283 - val_fscore_keras: 0.6849
Epoch 4/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1654 - maeOverFscore_keras: 1.4508 - fscore_keras: 0.6836 - val_loss: 0.1896 - val_maeOverFscore_keras: 1.5828 - val_fscore_keras: 0.6913
Epoch 5/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1646 - maeOverFscore_keras: 1.4445 - fscore_keras: 0.6840 - val_loss: 0.1891 - val_maeOverFscore_keras: 1.5863 - val_fscore_keras: 0.6900
Epoch 6/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1639 - maeOverFscore_keras: 1.4364 - fscore_keras: 0.6849 - val_loss: 0.1883 - val_maeOverFscore_keras: 1.5536 - val_fscore_keras: 0.6973
Epoch 7/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1634 - maeOverFscore_keras: 1.4301 - fscore_keras: 0.6857 - val_loss: 0.1926 - val_maeOverFscore_keras: 1.5472 - val_fscore_keras: 0.6992
Epoch 8/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1626 - maeOverFscore_keras: 1.4213 - fscore_keras: 0.6867 - val_loss: 0.1840 - val_maeOverFscore_keras: 1.5263 - val_fscore_keras: 0.7017
Epoch 9/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1621 - maeOverFscore_keras: 1.4158 - fscore_keras: 0.6872 - val_loss: 0.1862 - val_maeOverFscore_keras: 1.5953 - val_fscore_keras: 0.6878
Epoch 10/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1620 - maeOverFscore_keras: 1.4131 - fscore_keras: 0.6877 - val_loss: 0.1861 - val_maeOverFscore_keras: 1.5427 - val_fscore_keras: 0.6984
Epoch 11/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1616 - maeOverFscore_keras: 1.4103 - fscore_keras: 0.6877 - val_loss: 0.1873 - val_maeOverFscore_keras: 1.5410 - val_fscore_keras: 0.6974
Epoch 12/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1610 - maeOverFscore_keras: 1.4027 - fscore_keras: 0.6886 - val_loss: 0.1860 - val_maeOverFscore_keras: 1.5388 - val_fscore_keras: 0.7014
Epoch 13/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1603 - maeOverFscore_keras: 1.3945 - fscore_keras: 0.6898 - val_loss: 0.1845 - val_maeOverFscore_keras: 1.5113 - val_fscore_keras: 0.7042
=====Train Model_4 iteration====
Epoch 1/50
1710/1710 [==============================] - 234s 137ms/step - loss: 0.1593 - maeOverFscore_keras: 1.3844 - fscore_keras: 0.6906 - val_loss: 0.0000e+00 - val_maeOverFscore_keras: 0.0000e+00 - val_fscore_keras: 0.0000e+00
Epoch 2/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1584 - maeOverFscore_keras: 1.3757 - fscore_keras: 0.6914 - val_loss: 0.1837 - val_maeOverFscore_keras: 1.5308 - val_fscore_keras: 0.7015
Epoch 3/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1580 - maeOverFscore_keras: 1.3727 - fscore_keras: 0.6913 - val_loss: 0.1882 - val_maeOverFscore_keras: 1.5387 - val_fscore_keras: 0.6921
Epoch 4/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1574 - maeOverFscore_keras: 1.3648 - fscore_keras: 0.6923 - val_loss: 0.1858 - val_maeOverFscore_keras: 1.5141 - val_fscore_keras: 0.7050
Epoch 5/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1569 - maeOverFscore_keras: 1.3602 - fscore_keras: 0.6925 - val_loss: 0.1835 - val_maeOverFscore_keras: 1.5534 - val_fscore_keras: 0.6953
Epoch 6/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1564 - maeOverFscore_keras: 1.3564 - fscore_keras: 0.6927 - val_loss: 0.1855 - val_maeOverFscore_keras: 1.5160 - val_fscore_keras: 0.7041
Epoch 7/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1557 - maeOverFscore_keras: 1.3485 - fscore_keras: 0.6936 - val_loss: 0.1825 - val_maeOverFscore_keras: 1.5327 - val_fscore_keras: 0.6999
Epoch 8/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1552 - maeOverFscore_keras: 1.3430 - fscore_keras: 0.6942 - val_loss: 0.1830 - val_maeOverFscore_keras: 1.6123 - val_fscore_keras: 0.6768
Epoch 9/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1549 - maeOverFscore_keras: 1.3397 - fscore_keras: 0.6942 - val_loss: 0.1850 - val_maeOverFscore_keras: 1.5086 - val_fscore_keras: 0.7052
Epoch 10/50
1710/1710 [==============================] - 226s 132ms/step - loss: 0.1545 - maeOverFscore_keras: 1.3371 - fscore_keras: 0.6944 - val_loss: 0.1852 - val_maeOverFscore_keras: 1.5338 - val_fscore_keras: 0.6999
Epoch 11/50
1710/1710 [==============================] - 227s 132ms/step - loss: 0.1541 - maeOverFscore_keras: 1.3312 - fscore_keras: 0.6952 - val_loss: 0.1883 - val_maeOverFscore_keras: 1.5214 - val_fscore_keras: 0.7021
Epoch 12/50
1710/1710 [==============================] - 227s 133ms/step - loss: 0.1536 - maeOverFscore_keras: 1.3269 - fscore_keras: 0.6955 - val_loss: 0.1853 - val_maeOverFscore_keras: 1.5032 - val_fscore_keras: 0.7029

Process finished with exit code 0

강수량을 예측할 지점에서 멀리 있는 데이터일 수록 영향이 적어져 풀링레이어가 결과 예측에 방해가 될 것으로 판단

convolution과 residual block만으로 모델 설계