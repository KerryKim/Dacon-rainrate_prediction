{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2등 JM Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_absolute_error, confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input,BatchNormalization,LayerNormalization,Dropout,Conv2D,Conv2DTranspose, MaxPooling2D, Flatten,Activation, PReLU,concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\" # 사용할 GPU 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "# f1 score 계산\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "# MAE/f1\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return AIF_mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "def maeOverFscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
    "    return score\n",
    "\n",
    "# 실제값 0.1이상인 것에대한 MAE 값 계산\n",
    "def AIF_mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def mae_custom(y_true, y_pred):\n",
    "    score = tf.py_function(func=AIF_mae, inp=[y_true, y_pred], Tout=tf.float32,  name='mae_custom') \n",
    "    return score\n",
    "\n",
    "# classification 모델의 fscore 계산\n",
    "def fscore_(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    y_true = np.where(y_true >= 0.5, 1, 0)\n",
    "    y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def fscore_classification(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore_, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_classification')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(files, TRAIN=True):\n",
    "    '''\n",
    "    TRAIN : {True:훈련 데이터 로드, 결측값이 없는 온전한 데이터만 로드, False:테스트 데이터 로드, 모든 데이터 로드}\n",
    "    '''\n",
    "    tmp=[]\n",
    "    for file in tqdm(files[:]):\n",
    "        data = np.load(file).astype(np.float32)\n",
    "        if TRAIN:\n",
    "            if (data[:,:,-1]<0).sum()==0:                     # 결측값이 없는 데이터만 로드\n",
    "                tmp.append(data)\n",
    "        else:\n",
    "            tmp.append(data)\n",
    "    return np.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값이 없는 데이터만 Load\n",
    "train_files = sorted(glob.glob('data/train/*.npy'))[:]\n",
    "train_ = data_load(train_files)\n",
    "print('train shape :', train_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "한 픽셀의 target 값이 0.1 이상인 행들만 선택하여 밝기온도 차와 target의 상관계수가 높은 순으로 시각화 하여 확인\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 상관관계를 확인하기 위해서 먼저 pandas dataframe 형태로 만들어 준다.\n",
    "# dataframe으로 만들기 위해 reshape하는 부분이 endgame님 코드와 조금 다르다. 어쨌든 큰 램 메모리 용량이 필요할듯하다.\n",
    "df = pd.DataFrame(train_.reshape(-1, 15))\n",
    "df.columns=['BT1', 'BT2', 'BT3', 'BT4', 'BT5', 'BT6', 'BT7', 'BT8', 'BT9','type','GMI_lon', 'GMI_lat', 'DPR_lon', 'DPR_lat','target']\n",
    "BT_column=['BT1', 'BT2', 'BT3', 'BT4', 'BT5', 'BT6', 'BT7', 'BT8', 'BT9']\n",
    "\n",
    "# 밝기온도 차 컬럼 추가\n",
    "for i in range(9):\n",
    "    for j in range(i+1, 9):\n",
    "        df['{}_{}_diff'.format(BT_column[i],BT_column[j])] = df[BT_column[i]] - df[BT_column[j]]\n",
    "\n",
    "# target 값이 0.1 이상인 값만 추출\n",
    "df = df[df['target']>=0.1]\n",
    "\n",
    "# 상관계수 계산\n",
    "df_corr = df.corr()\n",
    "\n",
    "# 강수량 (target data)과 가장 큰 상관관계를 보이는 feature를 찾으면 된다.\n",
    "# 상관관계 값에 따라 가중치를 두는 것도 고려해볼만 할듯하다\n",
    "col = df_corr['target'].abs().sort_values(ascending=False).index\n",
    "\n",
    "# 시각화\n",
    "# 먼저 figure size를 정해줘야 한다.\n",
    "plt.rcParams['figure.figsize'] = [15, 1]\n",
    "for idx in range(0, 36, 9):\n",
    "    # value range가 엄청 넓기 때문에 범위를 지정해 줄 필요가 있을듯 하다. vmin=0, vmax=0.5\n",
    "    # sns.heatmap(데이터 전체, 우리가 주로 보고자 하는 행)\n",
    "    sns.heatmap(df_corr.loc[['target'],[k for k in col if 'diff' in k][idx:idx+9]].abs(), annot=True, linewidths=1, vmin=0, vmax=0.5)\n",
    "    \n",
    "    #  plt.xticks = 지정할 눈금들\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GMI(경도,위도) plot에서의 강수량을 시각화 하여 확인. (강수량 수치가 클수록 색이 진하고 크기가 큰 점)\n",
    "강수량이 중구난방 있지않고 비교적 밀집되어 있음을 알 수 있다. --> CNN 기반 딥러닝 모델 학습이 용이할 것 같다. --> Unet 모델 사용\n",
    "\n",
    "뭔가 뭉쳐있는 데이터를 K mean clustering으로 더 잘 featuring 할 수 있지 않을까 하는 생각도 든다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    pal = sns.dark_palette(\"palegreen\", as_cmap=True)\n",
    "    colormap = plt.cm.RdBu\n",
    "    sns.scatterplot(train_[idx,:,:,10].reshape(-1), train_[idx,:,:,11].reshape(-1), hue=train_[idx,:,:,-1].reshape(-1), size=train_[idx,:,:,-1].reshape(-1))\n",
    "    plt.xlabel('latitude')\n",
    "    plt.ylabel('longitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(9502)\n",
    "visualize(9503)\n",
    "visualize(9504)\n",
    "visualize(9505)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Engineering & Initial Modeling\n",
    "변수 선택 및 feature 생성 함수\n",
    "밝기온도 차이와 target 사이에서 상관계수가 높은 24개를 선택\n",
    "\n",
    "[(3,8), (5,8), (1,4), (6, 8), (5, 9), (3, 9), (4, 9), (4, 8), (7, 9), (7,8), (6, 9), (4, 6), (3, 6), (4, 5), (3, 4),(6, 7), (1, 8), (3, 5), (4, 7), (1, 9), (2, 8), (5, 7), (2, 9), (2, 4)]\n",
    "\n",
    "지표타입 feature은 get_dummy를 통해 one-hot encoding (이 부분도 특이한 듯. categorical feature도 활용)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feature(data, type_encoding=True):\n",
    "    #  밝기온도와 target의 상관 계수가 높은 24개를 생성.\n",
    "    '''\n",
    "    type_encoding : 지표 타입을 get_dummy를 사용하여 인코딩 할지 여부.\n",
    "    '''\n",
    "    tmp = []\n",
    "    # 위에 사용한 데이터를 쓰지 않고 여기서 새로 데이터 feature를 만들었다.\n",
    "    for bt in tqdm([(3,8), (5,8), (1,4), (6, 8), (5, 9), (3, 9), (4, 9), (4, 8), (7, 9), (7,8), (6, 9), (4, 6), (3, 6), (4, 5), (3, 4),\n",
    "              (6, 7), (1, 8), (3, 5), (4, 7), (1, 9), (2, 8), (5, 7), (2, 9), (2, 4)]):\n",
    "        tmp.append(data[:,:,:,bt[0]-1]-data[:,:,:,bt[1]-1])\n",
    "    tmp = np.array(tmp).reshape(-1, 40, 40, 24)\n",
    "\n",
    "    # type one-hot-encoding\n",
    "    if type_encoding:\n",
    "        TYPE = pd.get_dummies((data[:,:,:,9]//100).reshape(-1)).values\n",
    "        TYPE = TYPE.reshape(-1,40,40,4)\n",
    "        data = np.append(data[:,:,:,:9], data[:,:,:,10:],axis=-1)            # 원래 type을 제외\n",
    "        data = np.append(TYPE, data, axis=-1)                                 # 타입 합치기\n",
    "        data = np.append(tmp,data, axis=-1)                                   # 밝기온도 차이 합치기\n",
    "    else:\n",
    "        data = np.append(tmp,data, axis=-1)                                   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_enc = gen_feature(train_, type_encoding=True)\n",
    "train_non_type_enc = gen_feature(train_, type_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_enc.shape, train_non_type_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Agumentation 전 train과 valid 데이터 index로 사전 분리 (5Fold)\n",
    "train_idx, valid_idx로 저장해두고 훈련 시 불러와 사용.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold split\n",
    "# 다양한 시드를 사용한 모델 결과값을 이용해서 정규화 효과를 노린다.\n",
    "skf_seed42 = KFold(n_splits=5, shuffle= True, random_state=42)                 # kfold, random_state 설정\n",
    "skf_seed1030 = KFold(n_splits=5, shuffle= True, random_state=1030)                 # kfold, random_state 설정\n",
    "skf_seed1234 = KFold(n_splits=5, shuffle= True, random_state=1234)                 # kfold, random_state 설정\n",
    "\n",
    "kfolds_seed42 = []\n",
    "kfolds_seed1030 = []\n",
    "kfolds_seed1234 = []\n",
    "\n",
    "for train_idx, test_idx in skf_seed42.split(range(len(train_type_enc))):\n",
    "    kfolds_seed42.append((train_idx, test_idx))\n",
    "for train_idx, test_idx in skf_seed1030.split(range(len(train_type_enc))):\n",
    "    kfolds_seed1030.append((train_idx, test_idx))\n",
    "for train_idx, test_idx in skf_seed1234.split(range(len(train_type_enc))):\n",
    "    kfolds_seed1234.append((train_idx, test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Augmentation\n",
    "target에서 0.1 이상인 픽셀이 50개 이상일 경우 Data Augmentation 한다.\n",
    "horizontal flip\n",
    "vertical flip\n",
    "rotation 90, 180 270\n",
    "vertical flip + rotation90\n",
    "vertical flip + rotation270\n",
    "증량된 데이터는 기존 train 데이터와 합치고 섞는다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(X, y):\n",
    "    tmp = []\n",
    "    for idx in range(len(X)):\n",
    "        if (y[idx]>=0.1).sum()>=50:\n",
    "            tmp.append(idx)\n",
    "\n",
    "    X_train_aug90 = np.rot90(X[tmp], k=1, axes=(1,2))\n",
    "    y_train_aug90 = np.rot90(y[tmp], k=1, axes=(1,2))\n",
    "    X_train_aug180 = np.rot90(X[tmp], k=2, axes=(1,2))\n",
    "    y_train_aug180 = np.rot90(y[tmp], k=2, axes=(1,2))\n",
    "    X_train_aug270 = np.rot90(X[tmp], k=3, axes=(1,2))\n",
    "    y_train_aug270 = np.rot90(y[tmp], k=3, axes=(1,2))\n",
    "    X_train_flipV = np.flip(X[tmp],1)\n",
    "    y_train_flipV = np.flip(y[tmp],1)\n",
    "    X_train_flipH = np.flip(X[tmp],2)\n",
    "    y_train_flipH = np.flip(y[tmp],2)\n",
    "    X_train_flipV_90 = np.rot90(X_train_flipV, k=1, axes=(1,2))\n",
    "    y_train_flipV_90 = np.rot90(y_train_flipV, k=1, axes=(1,2))\n",
    "    X_train_flipV_270 = np.rot90(X_train_flipV, k=3, axes=(1,2))\n",
    "    y_train_flipV_270 = np.rot90(y_train_flipV, k=3, axes=(1,2))\n",
    "\n",
    "    # 증량 시킨 데이터들을 concatenate\n",
    "    X = np.concatenate([X, X_train_aug90, X_train_aug180, X_train_aug270, X_train_flipV, X_train_flipH, X_train_flipV_90, X_train_flipV_270])\n",
    "    y = np.concatenate([y, y_train_aug90, y_train_aug180, y_train_aug270, y_train_flipV, y_train_flipH, y_train_flipV_90, y_train_flipV_270])\n",
    "    del X_train_aug90, y_train_aug90, X_train_aug180, y_train_aug180, X_train_aug270, y_train_aug270, X_train_flipV, y_train_flipV, X_train_flipH, y_train_flipH, X_train_flipV_90, y_train_flipV_90, X_train_flipV_270, y_train_flipV_270\n",
    "    del tmp\n",
    "    # agumentation data와 기존 train 데이터 합친것을 shuffle 하여 데이터들을 섞어준다.\n",
    "    idx = np.array([k for k in range(len(X))]) \n",
    "    np.random.seed(1234)    \n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Tuning & Evaluation\n",
    "기본 Unet 구조를 사용하고 앞과 뒤 층을 수정\n",
    "input(40x40x41)을 UpSampling 하여 (80x80x41)을 Unet 기본 구조에 입력으로 한다.\n",
    "마지막 층(80x80x1)에서 Maxpooling하여 DownSampling 하고 (40x40x1)을 output으로 한다.\n",
    "Epochs : 100\n",
    "Batch Size : 456\n",
    "initial Learning rate : 0.001\n",
    "Learning rate Scheduler : Polynomial Decay (Cosine decay restarts도 번갈아 가며 사용하여 ensemble)\n",
    "Optimizer : Adam (SGD는 수렴속도가 느려 사용에 어려움이 있었음)\n",
    "Activation function : ELU 혹은 ReLU를 사용\n",
    "Output : Regression or Classification\n",
    "Upsampling Layer : Nearest Neighbor interporation\n",
    "dropout : 0.5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"])  # multi GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_regression(input_shape,activation='elu', weight=False):\n",
    "    '''\n",
    "    activation : relu or elu\n",
    "    weight : 로드할 모델 가중치의 여부\n",
    "    '''\n",
    "    drop = 0.5\n",
    "    with strategy.scope():\n",
    "        unit1 = 64\n",
    "        unit2 = 128\n",
    "        unit3 = 256\n",
    "        unit4 = 512\n",
    "        unit5 = 1024\n",
    "\n",
    "        inputs = Input(shape=(input_shape))\n",
    "        inputs_ = UpSampling2D(size=(2,2))(inputs)\n",
    "        conv1 = Conv2D(unit1, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs_)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Activation(activation)(conv1)\n",
    "        conv1 = Conv2D(unit1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Activation(activation)(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        pool1 = Dropout(drop)(pool1)\n",
    "        conv2 = Conv2D(unit2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Activation(activation)(conv2)\n",
    "        conv2 = Conv2D(unit2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Activation(activation)(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        pool2 = Dropout(drop)(pool2)\n",
    "        conv3 = Conv2D(unit3, 3, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = Activation(activation)(conv3)\n",
    "        conv3 = Conv2D(unit3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = Activation(activation)(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        pool3 = Dropout(drop)(pool3)\n",
    "        conv4 = Conv2D(unit4, 3, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        conv4 = Activation(activation)(conv4)\n",
    "        conv4 = Conv2D(unit4, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        conv4 = Activation(activation)(conv4)\n",
    "        drop4 = Dropout(drop)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "        \n",
    "        conv5 = Conv2D(unit5, 3, padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        conv5 = Activation(activation)(conv5)\n",
    "        conv5 = Conv2D(unit5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        conv5 = Activation(activation)(conv5)\n",
    "        drop5 = Dropout(drop)(conv5)\n",
    "\n",
    "        up6 = Conv2D(unit4, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        up6 = BatchNormalization()(up6)\n",
    "        up6 = Activation(activation)(up6)\n",
    "        merge6 = concatenate([drop4,up6], axis = 3)\n",
    "        merge6 = Dropout(drop)(merge6)\n",
    "        conv6 = Conv2D(unit4, 3, padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "        conv6 = Activation(activation)(conv6)\n",
    "        conv6 = Conv2D(unit4, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "        conv6 = Activation(activation)(conv6)\n",
    "        up7 = Conv2D(unit3, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        up7 = BatchNormalization()(up7)\n",
    "        up7 = Activation(activation)(up7)\n",
    "        merge7 = concatenate([conv3,up7], axis = 3)\n",
    "        merge7 = Dropout(drop)(merge7)\n",
    "        conv7 = Conv2D(unit3, 3, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "        conv7 = Activation(activation)(conv7)\n",
    "        conv7 = Conv2D(unit3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "        conv7 = Activation(activation)(conv7)\n",
    "        up8 = Conv2D(unit2, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        up8 = BatchNormalization()(up8)\n",
    "        up8 = Activation(activation)(up8)\n",
    "        merge8 = concatenate([conv2,up8], axis = 3)\n",
    "        merge8 = Dropout(drop)(merge8)\n",
    "        conv8 = Conv2D(unit2, 3, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = BatchNormalization()(conv8)\n",
    "        conv8 = Activation(activation)(conv8)\n",
    "        conv8 = Conv2D(unit2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        conv8 = BatchNormalization()(conv8)\n",
    "        conv8 = Activation(activation)(conv8)\n",
    "\n",
    "        up9 = Conv2D(unit1, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        up9 = BatchNormalization()(up9)\n",
    "        up9 = Activation(activation)(up9)\n",
    "        merge9 = concatenate([conv1,up9], axis = 3)\n",
    "        merge9 = Dropout(drop)(merge9)\n",
    "        conv9 = Conv2D(unit1, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = BatchNormalization()(conv9)\n",
    "        conv9 = Activation(activation)(conv9)\n",
    "        conv9 = Conv2D(unit1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = BatchNormalization()(conv9)\n",
    "        conv9 = Activation(activation)(conv9)\n",
    "        conv9 = Conv2D(2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = BatchNormalization()(conv9) \n",
    "        conv9 = Activation(activation)(conv9)\n",
    "        conv9 = MaxPooling2D(pool_size=(2, 2))(conv9)\n",
    "        conv9 = BatchNormalization()(conv9)\n",
    "        # regression output\n",
    "        regression = Conv2D(1, 1, activation = 'relu', name='regression')(conv9)\n",
    "        model = Model(inputs, regression)\n",
    "        if weight==False:\n",
    "            model.compile(loss='mae', optimizer=opt, metrics=[fscore_keras, maeOverFscore_keras, mae_custom] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
