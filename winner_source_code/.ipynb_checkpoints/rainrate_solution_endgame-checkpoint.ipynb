{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3등 endgame님 <Resnet 변형 모델 + Inception v3 기반 모델>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gc\n",
    "gc.collect()\n",
    "중간중간에 불필요한 메모리를 정리해 준다. \n",
    "(https://blog.naver.com/pica4star/221443758311)\n",
    "\n",
    "import pickle\n",
    "pickle은 프로그램상에서 사용하고 있는 데이터를 파일형태로 저장한다.\n",
    "(https://blog.naver.com/wjdwngkdsla/221978274816)\n",
    "(https://blog.naver.com/mania9899/221624931960)\n",
    "\n",
    "import seaborn as sns\n",
    "seaborn은 시각화 라이브러리이다. \n",
    "내가 파이참에서 model 구현시 이걸 써줘야 step이 아니라 epoch으로 모니터링되면서 진행됬다.\n",
    "(https://blog.naver.com/tkdzma8080/221793003678)\n",
    "'''\n",
    "\n",
    "# 파일관리 및 파일선택\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import SeparableConv2D, Input, Conv2D, Add, BatchNormalization, concatenate, AveragePooling2D, add, MaxPooling2D, Conv2DTranspose, Activation, Dropout, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # evaluation metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train = 'data/train/'\n",
    "dir_test = 'data/test/'\n",
    "UPPER = 50\n",
    "\n",
    "def make_dataset(dir_train, dir_test, UPPER):\n",
    "    # train dataset\n",
    "    train = []\n",
    "    train_y = []\n",
    "\n",
    "    for i in os.listdir(dir_train):\n",
    "        npy = np.load(dir_train + i)\n",
    "\n",
    "        # missing value 제거\n",
    "        if npy[:, :, -1].sum() < 0:\n",
    "            continue\n",
    "        \n",
    "        # 0.1이상 내린 픽셀이 UPPER 값 이상인 사진만\n",
    "        if (npy[:, :, -1] >= 0.1).sum() >= UPPER:\n",
    "            train.append(npy[:, :, :-1])\n",
    "            train_y.append(npy[:, :, -1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    train_y = np.array(train_y)\n",
    "    \n",
    "    '''\n",
    "    여기까진 내가 대회에서 해오던 방식과 유사하다\n",
    "    아래 구문은 RAM 용량을 효율적으로 사용하기 위해 train, train_y 리스트를 pickle로 저장후 list를 지우는 방법인듯하다.\n",
    "\n",
    "    pickle.dump protocol\n",
    "\n",
    "    파이썬 3.6을 쓴다면 프로토콜을 4를 써야 할 것 같다.\n",
    "    프로토콜이 음수 또는 HIGHEST_PROTOCOL로 지정되면 사용 가능한 최고 프로토콜 버전이 사용됩니다.\n",
    "    (https://ko.coder.work/so/python/73422)\n",
    "    '''\n",
    "    \n",
    "    with open(f'data/train{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train, f, protocol=4)\n",
    "\n",
    "    with open(f'data/train_y{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train_y, f, protocol=4)\n",
    "\n",
    "    del train\n",
    "    del train_y\n",
    "\n",
    "    # test dataset\n",
    "    test = []\n",
    "\n",
    "    for i in os.listdir(dir_test):\n",
    "        npy = np.load(dir_test + i)\n",
    "        test.append(npy)\n",
    "    test = np.array(test)\n",
    " \n",
    "    with open('data/test.pickle', 'wb') as f:\n",
    "        pickle.dump(test, f, protocol=4)\n",
    "    del test\n",
    "    \n",
    "make_dataset(dir_train, dir_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train50.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "# 0~9번채널만 사용\n",
    "train = train[:, :, :, :10]\n",
    "\n",
    "with open('data/train_y50.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "\n",
    "with open('data/test.pickle', 'rb') as f:\n",
    "    TEST = pickle.load(f)\n",
    "TEST = TEST[:, :, :, :10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 탐색적 자료분석 (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v 채널과 h 채널의 이미지를 만들어주는 과정. 각 채널의 value.sum()으로 구한다.\n",
    "def show_img(img):\n",
    "    ch15_v = 0\n",
    "    for i in [0,2,4,5,7]:\n",
    "        ch15_v += img[:,:,i]\n",
    "    ch15_h = 0\n",
    "    for i in [1,3,6,8]:\n",
    "        ch15_h += img[:,:,i]\n",
    "    ch15_v = ch15_v.reshape(40,40,1)\n",
    "    ch15_h = ch15_h.reshape(40,40,1)\n",
    "    img = np.concatenate([img, ch15_v], -1)\n",
    "    img = np.concatenate([img, ch15_h], -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "# image_dir[random.randrange(len(image_dir))] 을 통해 image dir에 있는 사진 중 임의로 한 장을 선택한다.\n",
    "image_dir = os.listdir('data/train/')\n",
    "image_sample = np.load(f'data/train/{image_dir[random.randrange(len(image_dir))]}')\n",
    "image_sample = show_img(image_sample)\n",
    "\n",
    "color_map = plt.cm.get_cmap('RdBu')\n",
    "color_map = color_map.reversed()\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(2,6,i+1)\n",
    "    plt.imshow(image_sample[:, :, i], cmap=color_map)\n",
    "    plt.title(f'ch_{i}', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,10)\n",
    "plt.imshow(image_sample[:,:,-3], cmap = color_map)\n",
    "plt.title('rain', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,11)\n",
    "plt.imshow(image_sample[:,:,-2], cmap = color_map)\n",
    "plt.title('v_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,12)\n",
    "plt.imshow(image_sample[:,:,-1], cmap = color_map)\n",
    "plt.title('h_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplots_adjust(top=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0:'v1',1:'h1',2:'v2',3:'h2',4:'v3',5:'v4',6:'h4',7:'v5',8:'h5',9:'surface',10:'target'\n",
    "correlation을 분석하기 위해서 데이터를 reshape해서 2차원 Dataframe을 만든다. (pandas의 corr() 함수를 이용)\n",
    "각 columns는 'v1'~'target'까지 10개의 columns를 갖는다. value가 너무 크다고 예상되면 log를 취한다.\n",
    "\n",
    "https://blog.naver.com/kiddwannabe/221763497317\n",
    "https://blog.naver.com/wtracer/221738979637\n",
    "\n",
    "'''\n",
    "\n",
    "train2 = train.reshape(train.shape[0] * train.shape[1] * train.shape[2], train.shape[3])\n",
    "train_y2 = train_y.reshape(train_y.shape[0] * train_y.shape[1] * train_y.shape[2], train_y.shape[3])\n",
    "train_y2 = np.log(train_y2+1)\n",
    "train2 = np.concatenate([train2, train_y2], -1)\n",
    "\n",
    "df_corr = pd.DataFrame(train2).reset_index(drop=True)\n",
    "del train2, train_y2\n",
    "\n",
    "df_corr = df_corr.iloc[400::1600, :] # every other element, starting at index 400, 400부터 시작해서 1600만큼 건너뜀\n",
    "df_corr = df_corr.reset_index(drop=True)\n",
    "df_corr = df_corr.rename(columns={0:'v1',1:'h1',2:'v2',3:'h2',4:'v3',5:'v4',\n",
    "                                  6:'h4',7:'v5',8:'h5',9:'surface',10:'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "v, h 채널의 합 또는 차이가 도움을 줄 지 확인을 해봤습니다. 아주 조금 상관관계가 상승하는 것을 확인할 수 있었고,\n",
    "두 피쳐간의 합과 차이는 45도 회전변환 시의 상관관계와 같다는 아이디어에서 착안, 각각 30도, 45, 60도 회전변환해보았습니다.\n",
    "그 결과 v1-h1, v2-h2, v4-h4는 45도 회전변환시 아주 조금 상관관계가 증가하였고,\n",
    "v5-h5는 30도 변환 시 상관관계가 매우 크게 증가하는 것을 확인하여 회전변환한 피쳐를 사용하였습니다.\n",
    "'''\n",
    "\n",
    "'''\n",
    "df_corr['ch1_rot1'] = df_corr['v1'] * np.cos(np.pi / 4) + df_corr['h1'] * np.sin(np.pi / 4)\n",
    "df_corr['ch1_rot2'] = df_corr['v1'] * np.cos(np.pi / 4) - df_corr['h1'] * np.sin(np.pi / 4)\n",
    "\n",
    "df_corr['ch2_rot1'] = df_corr['v2'] * np.cos(np.pi / 4) + df_corr['h2'] * np.sin(np.pi / 4)\n",
    "df_corr['ch2_rot2'] = df_corr['v2'] * np.cos(np.pi / 4) - df_corr['h2'] * np.sin(np.pi / 4)\n",
    "\n",
    "df_corr['ch4_rot1'] = df_corr['v4'] * np.cos(np.pi / 4) + df_corr['h4'] * np.sin(np.pi / 4)\n",
    "df_corr['ch4_rot2'] = df_corr['v4'] * np.cos(np.pi / 4) - df_corr['h4'] * np.sin(np.pi / 4)\n",
    "\n",
    "df_corr['ch5_rot1'] = df_corr['v5'] * np.cos(np.pi / 6) + df_corr['h5'] * np.sin(np.pi / 6)\n",
    "df_corr['ch5_rot2'] = df_corr['v5'] * np.cos(np.pi / 6) - df_corr['h5'] * np.sin(np.pi / 6)\n",
    "\n",
    "df_corr.corr()['target']\n",
    "\n",
    "del df_corr\n",
    "gc.collect()\n",
    "\n",
    "# 사실 좀 이해 안가는 내용\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 값 log 변환\n",
    "train_y = np.log(train_y+1)\n",
    "\n",
    "\n",
    "# v채널/h채널 sum한 feature 추가\n",
    "def channel_sum(data):\n",
    "    data_v = data[:, :, :, 0].copy() + data[:, :, :, 2].copy() + data[:, :, :, 4].copy() + data[:, :, :, 5].copy() +data[:, :, :, 7].copy()\n",
    "    data_h = data[:, :, :, 1].copy() + data[:, :, :, 3].copy() + data[:, :, :, 6].copy() + data[:, :, :, 8].copy()\n",
    "\n",
    "    data_v = data_v.reshape(data_v.shape[0], data_v.shape[1], data_v.shape[2], 1)\n",
    "    data_h = data_h.reshape(data_h.shape[0], data_h.shape[1], data_h.shape[2], 1)\n",
    "\n",
    "    data = np.concatenate([data, data_v.copy()], -1)\n",
    "    data = np.concatenate([data, data_h.copy()], -1)\n",
    "\n",
    "    return data\n",
    "\n",
    "train = channel_sum(train)\n",
    "TEST = channel_sum(TEST)\n",
    "\n",
    "\n",
    "# 9번 채널(지표 타입:surface)만 min-max scaling\n",
    "train[:, :, :, 9] = train[:, :, :, 9] / 322\n",
    "TEST[:, :, :, 9] = TEST[:, :, :, 9] / 322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1-h1, v2-h2, v4-h4, v5-h5 채널을 cos과 sin으로 회전한 feature를 학습 input으로 사용 (1~8채널 그대로 사용하지 않는듯)\n",
    "def rotation(data):\n",
    "    v1_m_h1 = data[:, :, :, 0] * np.cos(np.pi / 4) - data[:, :, :, 1] * np.sin(np.pi / 4)\n",
    "    v1_p_h1 = data[:, :, :, 0] * np.cos(np.pi / 4) + data[:, :, :, 1] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 0] = v1_m_h1\n",
    "    data[:, :, :, 1] = v1_p_h1\n",
    "    del v1_m_h1\n",
    "    del v1_p_h1\n",
    "\n",
    "    v2_m_h2 = data[:, :, :, 2] * np.cos(np.pi / 4) - data[:, :, :, 3] * np.sin(np.pi / 4)\n",
    "    v2_p_h2 = data[:, :, :, 2] * np.cos(np.pi / 4) + data[:, :, :, 3] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 2] = v2_m_h2\n",
    "    data[:, :, :, 3] = v2_p_h2\n",
    "    del v2_m_h2\n",
    "    del v2_p_h2\n",
    "\n",
    "    v4_p_h4_30 = data[:, :, :, 5] * np.cos(np.pi / 4) + data[:, :, :, 6] * np.sin(np.pi / 4)\n",
    "    v4_m_h4_30 = data[:, :, :, 5] * np.cos(np.pi / 4) - data[:, :, :, 6] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 5] = v4_p_h4_30\n",
    "    data[:, :, :, 6] = v4_m_h4_30\n",
    "    del v4_p_h4_30\n",
    "    del v4_m_h4_30\n",
    "\n",
    "    v5_p_h5_30 = data[:, :, :, 7] * np.cos(np.pi / 6) + data[:, :, :, 8] * np.sin(np.pi / 6)\n",
    "    v5_m_h5_30 = data[:, :, :, 7] * np.cos(np.pi / 6) - data[:, :, :, 8] * np.sin(np.pi / 6)\n",
    "    data[:, :, :, 7] = v5_p_h5_30\n",
    "    data[:, :, :, 8] = v5_m_h5_30\n",
    "    del v5_p_h5_30\n",
    "    del v5_m_h5_30\n",
    "\n",
    "    return data\n",
    "\n",
    "train = rotation(train)\n",
    "TEST = rotation(TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # model training & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet\n",
    "def resnet_model(shape):\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    conv0 = Conv2D(256, kernel_size=1, strides=1, padding='same',\n",
    "                   activation='relu', kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    bn = BatchNormalization()(conv0)\n",
    "    conv = Conv2D(128, kernel_size=2, strides=1, padding='same',\n",
    "                  activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([conv0, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    conv = Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                  activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    # 5에서 9로 증가\n",
    "    for i in range(9):\n",
    "        bn = BatchNormalization()(concat)\n",
    "        conv = Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu', kernel_initializer='he_normal')(bn)\n",
    "        concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    outputs = Conv2D(1, kernel_size=1, strides=1, padding='same',\n",
    "                     activation='relu', kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation\n",
    "\n",
    "train = np.concatenate([train, train_y], -1)\n",
    "\n",
    "train1 = np.rot90(train, 1, (1,2))\n",
    "train2 = np.rot90(train, 2, (1,2))\n",
    "train3 = np.rot90(train, 3, (1,2))\n",
    "train_lr = np.fliplr(train)\n",
    "train_ud = np.flipud(train)\n",
    "\n",
    "train = np.vstack([train, train1])\n",
    "del train1\n",
    "\n",
    "train = np.vstack([train, train2])\n",
    "del train2\n",
    "\n",
    "train = np.vstack([train, train3])\n",
    "del train3\n",
    "\n",
    "train = np.vstack([train, train_lr])\n",
    "del train_lr\n",
    "\n",
    "train = np.vstack([train, train_ud])\n",
    "del train_ud\n",
    "\n",
    "train_y = train[:, :, :, -1].copy()\n",
    "train_y = train_y.reshape(train_y.shape[0], train_y.shape[1], train_y.shape[2], 1)\n",
    "train = train[:,:,:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split(train, train_y, test_size=0.025, random_state=SEED)\n",
    "\n",
    "model_number = 0\n",
    "history = []\n",
    "scores = []\n",
    "\n",
    "# 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "FOLD = 100\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = resnet_model(train.shape[1:])\n",
    "    model.summary()\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "    es = EarlyStopping(patience=9, verbose=1)\n",
    "    mc = ModelCheckpoint(f'model1_best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "    csv_logger = CSVLogger(f'training_{model_number}.csv')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs = 53, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp, csv_logger])\n",
    "    \n",
    "    # 에폭 53번까지 돌리다가 실수로 중단시켜\n",
    "    break\n",
    "    \n",
    "# 14번 더 돌렸습니다. (mae가 개선되지 않아 14번 돌리다가 도중에 중단시켰습니다.)\n",
    "model.fit(x_train, y_train, epochs = 14, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp, csv_logger])\n",
    "model.load_weights(f'model1_best_{model_number}.h5')\n",
    "res = model.predict(TEST)\n",
    "result = (np.exp(res)-1)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('model1_resnet.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception\n",
    "# https://norman3.github.io/papers/docs/google_inception.html\n",
    "# 인셉션 v3 모델 앞 부분에서 착안하여 모델을 만들어 보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(shape_, LOOP):\n",
    "    \n",
    "    input_ = Input(shape=shape_)\n",
    "    activation_ = 'relu'\n",
    "    \n",
    "    bn = BatchNormalization()(input_)\n",
    "    conv0 = Conv2D(256, kernel_size=1, strides=1, padding='same',\n",
    "                   activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    bn = BatchNormalization()(conv0)\n",
    "    conv = Conv2D(128, kernel_size=2, strides=1, padding='same',\n",
    "                  activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([conv0, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    conv = Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                  activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    for i in range(LOOP):\n",
    "        bn = BatchNormalization()(concat)\n",
    "        x_1 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "\n",
    "        x_2 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "        x_2 = Conv2D(32, 3, padding='same', activation=activation_)(x_2)\n",
    "\n",
    "        x_3 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "        x_3 = Conv2D(32, 3, padding='same', activation=activation_)(x_3)\n",
    "        x_3 = Conv2D(32, 3, padding='same', activation=activation_)(x_3)\n",
    "\n",
    "        x_4 = AveragePooling2D(\n",
    "            pool_size=(3, 3), strides=1, padding='same')(bn)\n",
    "        x_4 = Conv2D(32, 1, padding='same', activation=activation_)(x_4)\n",
    "\n",
    "        concat = concatenate([x_1, x_2, x_3, x_4])\n",
    "    \n",
    "    bn = BatchNormalization()(concat)\n",
    "\n",
    "    outputs = Conv2D(1, kernel_size=1, strides=1, padding='same',\n",
    "                     activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 0\n",
    "history = []\n",
    "scores = []\n",
    "\n",
    "# 위에서 설명했듯이 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "FOLD = 100\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = inception(train.shape[1:] , 5)\n",
    "    model.summary()\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    \n",
    "    es = EarlyStopping(patience=9, verbose=1)\n",
    "    mc = ModelCheckpoint(f'model2_best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs = 85, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp])\n",
    "    model.load_weights(f'model2_best_{model_number}.h5')\n",
    "    res = model.predict(TEST)\n",
    "\n",
    "    break\n",
    "    \n",
    "result = (np.exp(res)-1)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('model2_inception.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
