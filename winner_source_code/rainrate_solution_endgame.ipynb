{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈 불러오기\n",
    "\n",
    "Ref1)\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "중간중간에 불필요한 메모리를 정리해 준다. (https://blog.naver.com/pica4star/221443758311)\n",
    "\n",
    "Ref2)\n",
    "import pickle\n",
    "\n",
    "pickle은 프로그램상에서 사용하고 있는 데이터를 파일형태로 저장한다.\n",
    "(https://blog.naver.com/wjdwngkdsla/221978274816)\n",
    "(https://blog.naver.com/mania9899/221624931960)\n",
    "\n",
    "Ref3)\n",
    "import seaborn as sns\n",
    "\n",
    "seaborn은 시각화 라이브러리이다. 그리고 이걸 써줘야 에폭으로 학습이 진행됬다.\n",
    "(https://blog.naver.com/tkdzma8080/221793003678)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일관리 및 파일선택\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import SeparableConv2D, Input, Conv2D, Add, BatchNormalization, concatenate, AveragePooling2D, add, MaxPooling2D, Conv2DTranspose, Activation, Dropout, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    \n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "Ref1)\n",
    "\n",
    "pickle.dump protocol\n",
    "\n",
    "파이썬 3.6을 쓴다면 프로토콜을 4를 써야 할 것 같다.\n",
    "프로토콜이 음수 또는 HIGHEST_PROTOCOL로 지정되면 사용 가능한 최고 프로토콜 버전이 사용됩니다.\n",
    "(https://ko.coder.work/so/python/73422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train = 'data/train/'\n",
    "dir_test = 'data/test/'\n",
    "UPPER = 50\n",
    "\n",
    "def make_dataset(dir_train, dir_test, UPPER):\n",
    "    # train dataset\n",
    "    train = []\n",
    "    train_y = []\n",
    "\n",
    "    for i in os.listdir(dir_train):\n",
    "        npy = np.load(dir_train + i)\n",
    "\n",
    "        # missing value 제거\n",
    "        if npy[:, :, -1].sum() < 0:\n",
    "            continue\n",
    "        \n",
    "        # 0.1이상 내린 픽셀이 UPPER 값 이상인 사진만\n",
    "        if (npy[:, :, -1] >= 0.1).sum() >= UPPER:\n",
    "            train.append(npy[:, :, :-1])\n",
    "            train_y.append(npy[:, :, -1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    train_y = np.array(train_y)\n",
    "    \n",
    "    \n",
    "    # 여기까진 내가 대회에서 해오던 방식과 유사하다\n",
    "    # 아래 구문은 RAM 용량을 효율적으로 사용하기 위해 train, train_y 리스트를 pickle로 저장후 list를 지우는 방법인듯하다.\n",
    "\n",
    "    with open(f'data/train{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train, f, protocol=4)\n",
    "\n",
    "    with open(f'data/train_y{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train_y, f, protocol=4)\n",
    "\n",
    "    del train\n",
    "    del train_y\n",
    "\n",
    "    # test dataset\n",
    "    test = []\n",
    "\n",
    "    for i in os.listdir(dir_test):\n",
    "        npy = np.load(dir_test + i)\n",
    "        test.append(npy)\n",
    "    test = np.array(test)\n",
    "\n",
    "    with open('data/test.pickle', 'wb') as f:\n",
    "        pickle.dump(test, f, protocol=4)\n",
    "    del test\n",
    "    \n",
    "make_dataset(dir_train, dir_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train50.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "# 0~9번채널만 사용\n",
    "train = train[:, :, :, :10]\n",
    "\n",
    "with open('data/train_y50.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "\n",
    "with open('data/test.pickle', 'rb') as f:\n",
    "    TEST = pickle.load(f)\n",
    "TEST = TEST[:, :, :, :10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 탐색적 자료분석 (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅍ 채널과 h 채널의 이미지를 만들어주는 과정. 각 채널의 value.sum()으로 구한다.\n",
    "def show_img(img):\n",
    "    ch15_v = 0\n",
    "    for i in [0,2,4,5,7]:\n",
    "        ch15_v += img[:,:,i]\n",
    "    ch15_h = 0\n",
    "    for i in [1,3,6,8]:\n",
    "        ch15_h += img[:,:,i]\n",
    "    ch15_v = ch15_v.reshape(40,40,1)\n",
    "    ch15_h = ch15_h.reshape(40,40,1)\n",
    "    img = np.concatenate([img, ch15_v], -1)\n",
    "    img = np.concatenate([img, ch15_h], -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "# image_dir[random.randrange(len(image_dir))] 을 통해 image dir에 있는 사진 중 임의로 한 장을 선택한다.\n",
    "image_dir = os.listdir('data/train/')\n",
    "image_sample = np.load(f'data/train/{image_dir[random.randrange(len(image_dir))]}')\n",
    "image_sample = show_img(image_sample)\n",
    "\n",
    "color_map = plt.cm.get_cmap('RdBu')\n",
    "color_map = color_map.reversed()\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(2,6,i+1)\n",
    "    plt.imshow(image_sample[:, :, i], cmap=color_map)\n",
    "    plt.title(f'ch_{i}', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,10)\n",
    "plt.imshow(image_sample[:,:,-3], cmap = color_map)\n",
    "plt.title('rain', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,11)\n",
    "plt.imshow(image_sample[:,:,-2], cmap = color_map)\n",
    "plt.title('v_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,12)\n",
    "plt.imshow(image_sample[:,:,-1], cmap = color_map)\n",
    "plt.title('h_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplots_adjust(top=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.reshape(train.shape[0] * train.shape[1] * train.shape[2], train.shape[3])\n",
    "train_y2 = train_y.reshape(train_y.shape[0] * train_y.shape[1] * train_y.shape[2], train_y.shape[3])\n",
    "train_y2 = np.log(train_y2+1)\n",
    "train2 = np.concatenate([train2, train_y2], -1)\n",
    "\n",
    "df_corr = pd.DataFrame(train2).reset_index(drop=True)\n",
    "del train2, train_y2\n",
    "df_corr = df_corr.iloc[400::1600, :] # every other element, starting at index 400, 400부터 시작해서 1600만큼 건너뜀\n",
    "df_corr = df_corr.reset_index(drop=True)\n",
    "df_corr = df_corr.rename(columns={0:'v1',1:'h1',2:'v2',3:'h2',4:'v3',5:'v4',\n",
    "                                  6:'h4',7:'v5',8:'h5',9:'surface',10:'target'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
